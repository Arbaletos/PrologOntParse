# PrologOntParse  

Набор сценариев для автоматической популяции баз знаний и автоматического извлечения отношений на шаблонах, а также генерации вопросов по базе знаний.

## Исходные файлы
`txt/` -- Исходные файлы с текстами лекций, необработанным (`lecion.txt`) и формализованным (`lecion_formal.txt`)  
`kb/` -- Базы знаний в формате языка Prolog: набора правил (`ontology.pr`), базы знаний, созданной вручную (`kb_hand.pr`) и автоматически (`kb_auto.pr`)  
`templates/` -- Файлы с шаблонами для автоматического извлечения терминов и отношений: для извлечения только терминов (`term.txt`), терминов и отношений (`templates.txt`), шаблоны для формализованного текста (`formal.txt`)  
`requirements.txt` -- Зависимости для среды Python, `pip install -r requirements.txt`  
`morph.py` -- Модуль морфологического разбора и склонения слов/словосочетаний для русского языка (на основе `pymorphy2`)  

## Популяция баз знаний
`ont_to_text.py` -- Генерирует формализованный естественный текст по файлу .pr с онтологией: `python ont_to_text.py input_file output_file`  
`ont_parse.py` -- Код для генерации онтологии по тексту адаптированной лекции: `python ont_parse.py input_file output_file`  
`compile_ont.py` -- Собирает полную онтологию из файла с отношениями (`ontology.pr`) и файла с базой знаний: `python compile_ont.py ontology.pr kb.pr output.pr`.
Полную онтологию можно закинуть в интерпретатор языка пролог (например, https://swish.swi-prolog.org/) и позадавать вопросы, пользуясь синтаксисом языка. 
Парсер Онтологий работает на очень узком количестве шаблонов, и посему нормально обработать может только формализованные тексты.

## Извлечение терминов и отношений
`term_extract.py` -- простенькая извлекалочка на регулярных выражениях и захардкоженных ключевых словах
`gram_parse.py` -- извлекалочка посложнее. Кушает файлец с шаблонами, текст лекции, и опциональный файл для вывода в формате .tsv: `python gram_parse.py templates_file input_file [output_file]`. Подходящие файлы -- `templates`, `term_extract`, `formal`.  
`gram_parse.py -r` -- ещё более сложная извлекалочка, запускаемая из того же файла (сами классы извлекалочек лежат в `parser.py`. Кушает не просто шаблоны, а файл с целой грамматикой (например, `gram.txt`), которую пытается разобрать в одностороннем порядке. Из дополнительных особенностей -- пытается разобрать извлекаемые словосочетания по частям речи, выбрасывая те варианты, которые не подходят. Программулина не пытается найти самый лучший вариант разбора, и берет лишь первый удачный, так что порядок правил в файле с грамматиками важен. 

### Формат шаблонов:
**$TERM**, **$HYPONYM** и иже с ними -- нетерминальные символы (их значение пытается определить программулина), определение в терминах re: `'\$\w+'`   
**$*** -- нетерминальный символ, означающий любую последовательность, которая нас не интересует.  
Все остальные символы -- терминалы, совпадение с коими ищется в тексте.
Каждый терминал, обнаруженный в тексте, программулина пытается обработать и извлечь из него именную группу (совокупность согласованных прилагательных и существительных вместе с паравозом относительных существительных). Не рекомендуется употреблять **$*** непосредственно перед или после нетерминала, так как это порождает неоднозначность.
Пример: шаблон `$TERM - это $*` ищет все фразы, содержащие ключевую фразу " - это ", и извлекает левую часть этих фраз в качестве термина.
### Дополнительные правила для описания грамматики:  
**$EXPRESSION::-** -- все правила начинаются с указания, какой нетерминал описывается в настоящем правиле.
**$ADJF**, **$NOUN** -- для уровня отдельных слов можно указывать, какой части речи должны быть слова, используя тэги OpenCorpora. Также через нижний слэш можно добавлять ограничения по граммемам, например, **$NOUN_gent_sing** для существительного родительного падежа единственного числа.

### Farenda:
* Пока Анализатор онтологий и Извлекатор терминов не сильно связаны -- надо переобработать выход Извлекатора, чтобы он делал правильные онтологические выводы.
* В теории, для этой задачи можно приспособить нормальные грамматики, но по факту обычные формальные грамматики слишком 'формальны' для нас -- нам желательно иметь более покладистые методы.
* Также, весь этот репозиторий, скорее всего, -- один большой велосипед, но чего не сделаешь ради того, чтобы разобраться в проблеме.
* Возможны различные варианты разбора, во-первых могут сработать несколько правил за раз -- надо решить, какое использовать (сейчас используется первое подходящее по списку). Во вторых наложение масок на предложение может быть совершенно разными способами, что пока не учитывается
* Была добавлена возможность фильтрации по поз-тегам, но убрана возможность автоматической нормализации, что и хорошо и плохо.

## Генерация вопросов по базе знаний
`demandanto.py` -- описывает класс Demandanto, умеющий задавать вопросы о термине, содержащемся в базе знаний вышеописанного формата .pr. Пока что задает пять классов вопросов -- что такое (вопрос об гиперониме и атрибутах), на какие виды делится (вопрос о гипонимах), из чего состоит (вопрос о меронимах), куда входит (вопрос о голонимах), какие примеры существуют (вопрос о экземплярах). Пока что шаблоны для построения вопросов захардкожены в классе, возможность вынести их во внешний файл обсуждается, так как необходимо обеспечить инструментарий для контроля согласования служебных слов в генерируемых шаблонах (ekz. котор-ый/-ая/-ое).  
`__init__` кушает путь к файлу с базой знаний (например, kb/kb_hand.pr), `ask` генерирует пары вопрос-ответ для переданного в качестве аргумента термина, если, конечно, он есть в базе. Запуск сценария в качестве main, т.б. `demandanto.py kb/kb_hand.pr` запускает генерацию вопросов по всем терминам, обнаруженным в переданной в качестве аргумента базе знаний.  
Для реализации более сложных вариантов использования и генерации более сложных вопросов необходимо прикрутить таки пролог, что пока не было сделано.
 
## Формат базы знаний
Наша база знаний описывается в файлике формата .pr посредством синтакса языка Prolog, с которым можно ознакомиться на https://swish.swi-prolog.org/ соответствующих ресурсах.
#### О строении непосредственно нашей базы:  
В файлике `kb_.pr` описываются факты. В файлике `ontology.pr` описываются свойства семантических отношений, которые мы используем, то есть правила логического вывода новых фактов на основе имеющихся. Необходимость разделения файлов вызвана тем, что базу знаний мы можем получить по-разному и она будет различаться для разных данных, а правила отношений одинаковы. Чтобы собрать файл базы знаний и онтологии в один файл для ваших нужно можно применить сценарий `compile_ont.py`.  
#### Cемантические отношения
 `term(X).` -- говорит о том, что X является термином.  
 `syn(X, Y).` -- говорит о том, что Y является синонимом X.  
 `mer(X, Y).` -- говорит о том, что Y является меронимом (составной частью) X.  
 `hyp(X, Y).` -- говорит о том, что Y является гипонимом (потомком) X.  
 `inst(X, Y).` -- говорит о том, что Y является примером (экземпляром) X.  
 `attr(X, Y).` -- говорит о том, что X имеет атрибут(свойство) Y. По факту Y может быть любой описывающей строчкой на естественном языке.  
 Обратные семантические отношения (холонимия, гиперонимия) получаются из этих инверсией аргументов в предикате.
 Обдумывается также добавление тернарного отношения `rel(X, Y, Z)`, говорящего о том, что X и Y имеют связь Z, однако в настоящий момент подобные отношения представленными в репозитории сценариями не обрабатываются и в базе знаний их нет.  
